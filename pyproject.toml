[project]
name = "benchmark"
version = "0.1.0"
description = "Batch LLM evaluation tool for persistent memory risks"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "aiohttp>=3.13.2",
    "anthropic>=0.73.0",
    "cffi>=2.0.0",
    "dspy-ai>=2.6.0",
    "google-auth>=2.43.0",
    "google-cloud-aiplatform>=1.38",
    "google-cloud-storage>=3.5.0",
    "google-genai==1.51.0",
    "litellm[proxy]==1.80.0",
    "matplotlib>=3.10.7",
    "mlflow>=2.18.0",
    "nest-asyncio>=1.6.0",
    "openai>=1.107.2",
    "plotly>=6.5.0",
    "pydantic>=2.11.9",
    "python-dotenv>=1.2.1",
    "requests>=2.32.5",
    "rich>=13.7.1",
    "tenacity>=9.0.0",
    "tqdm>=4.67.1",
]

[project.scripts]
benchmark = "benchmark.eval_cli:main"
failure-rates = "analysis.failure_rates:main"
optimize-prompt = "benchmark.optimize_cli:main"

[dependency-groups]
dev = [
    "ipykernel>=6.30.1",
    "pre-commit>=4.0.0",
    "pytest>=8.3.3",
    "pytest-asyncio>=0.24.0",
    "ruff>=0.14.3",
    "ty>=0.0.8",
]

[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
package-dir = {"" = "src", "analysis" = "analysis"}
packages = ["benchmark", "benchmark.providers", "benchmark.execution", "analysis"]
